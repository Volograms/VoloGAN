{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VoloGAN_Drawings.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNh6nz9ZhjG0+84xTIblVjc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaKi1309/VoloGAN/blob/master/VoloGAN_Drawings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SJAFf5_s4SD"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(-2,1,0.1)   # start,stop,step\n",
        "relu = np.where(x > 0, x, 0)\n",
        "leakyRelu = np.where(x > 0, x, 0.2*x)\n",
        "elu = np.where(x > 0, x, 0.5* (np.exp(x)-1))\n",
        "\n",
        "d_relu = np.where(x > 0, 1, 0)\n",
        "d_leakyRelu = np.where(x > 0, 1, 0.2)\n",
        "d_elu = np.where(x > 0, 1, elu + 0.5)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = plt.figure(figsize=[15,5])\n",
        "fig.suptitle(\"Activation Comparison\", fontsize=20)\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.set_title('Activation Function')\n",
        "ax1.plot(x,relu,label=\"relu (x)\",linestyle=\"--\")\n",
        "ax1.plot(x,leakyRelu,label=\"leaky_relu (x, 0.2)\",linestyle=\"--\")\n",
        "ax1.plot(x,elu,label=\"elu (x, 0.5)\",linestyle=\"--\")\n",
        "ax1.set_xlabel(\"x\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "ax1.grid(linestyle=':', linewidth=1,)\n",
        " \n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.set_title('Derivitive Activation Function')\n",
        "ax2.plot(x,d_relu,label=\"relu'(x)\",linestyle=\"--\")\n",
        "ax2.plot(x,d_leakyRelu,label=\"leaky_relu'(x, 0.2)\",linestyle=\"--\")\n",
        "ax2.plot(x,d_elu,label=\"elu'(x, 0.5)\",linestyle=\"--\")\n",
        "ax2.set_xlabel(\"x\")\n",
        "ax2.legend(loc=\"upper left\")\n",
        "ax2.grid(linestyle=':', linewidth=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTU5ua3YtOpD"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(-4,4,0.005)   # start,stop,step\n",
        "tanh = np.tanh(x)\n",
        "sigmoid = 1/(1 + np.exp(-x))\n",
        "hard_sigmoid = np.where(x < (-2.5),0,np.where(x > 2.5 , 1, 0.2 * x + 0.5))\n",
        "\n",
        "d_tanh = 1 - np.tanh(x)**2\n",
        "d_sigmoid = sigmoid * (1-sigmoid)\n",
        "d_hard_sigmoid = np.where(x < (-2.5),0,np.where(x > 2.5 , 0, 0.2 ))\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = plt.figure(figsize=[15,5])\n",
        "fig.suptitle(\"Activation Comparison\", fontsize=20)\n",
        "\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.set_title('Activation Function')\n",
        "ax1.plot(x,tanh,label=\"tanh (x)\",linestyle=\"-\")\n",
        "ax1.plot(x,sigmoid,label=\"sigmoid (x)\",linestyle=\"-\")\n",
        "ax1.plot(x,hard_sigmoid,label=\"hard_sigmoid (x)\",linestyle=\"-\")\n",
        "ax1.set_xlabel(\"x\")\n",
        "ax1.legend(loc=\"upper left\")\n",
        "ax1.grid(linestyle=':', linewidth=1,)\n",
        "\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.set_title('Derivitive Activation Function')\n",
        "ax2.plot(x,d_tanh,label=\"tanh'(x)\",linestyle=\"-\")\n",
        "ax2.plot(x,d_sigmoid,label=\"sigmoid'(x)\",linestyle=\"-\")\n",
        "ax2.plot(x,d_hard_sigmoid,label=\"hard_sigmoid'(x)\",linestyle=\"-\")\n",
        "ax2.set_xlabel(\"x\")\n",
        "ax2.legend(loc=\"upper left\")\n",
        "ax2.grid(linestyle=':', linewidth=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJAoF4VZZTUo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "class HeUniformLeakyRelu(tf.keras.initializers.Initializer):\n",
        "  def __init__(self,alpha = 0.3):\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def __call__(self, shape, dtype=None, **kwargs):\n",
        "    n = np.prod(shape)\n",
        "    limit = np.sqrt(6/((1+self.alpha**2)*n))\n",
        "    return tf.random.uniform( shape,-limit,limit, dtype=dtype)\n",
        "\n",
        "  def get_config(self):  # To support serialization\n",
        "    return {\"alpha\": self.alpha}\n",
        "\n",
        "class HeNormalLeakyRelu(tf.keras.initializers.Initializer):\n",
        "  def __init__(self, alpha = 0.3):\n",
        "    \n",
        "    self.alpha = alpha\n",
        "\n",
        "  def __call__(self, shape, dtype=None, **kwargs):\n",
        "    n = np.prod(shape)\n",
        "    std = np.sqrt(2)/np.sqrt((1+self.alpha**2)*n)\n",
        "    return tf.random.truncated_normal(shape, mean=0, stddev=std, dtype=dtype)\n",
        "\n",
        "  def get_config(self):  # To support serialization\n",
        "    return {\"alpha\": self.alpha}\n",
        "\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "def testmodel(initializer):\n",
        "  input = tf.keras.layers.Input(shape=(512,512,4))\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 4, kernel_size = (3, 3), kernel_initializer = initializer,\n",
        "      use_bias = False,name=\"Layer1\")(input)\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 8, kernel_size = (3, 3), strides=(2,2), kernel_initializer = initializer, \n",
        "      use_bias = False,name=\"Layer2\")(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 8, kernel_size = (3, 3), kernel_initializer = initializer,\n",
        "      use_bias = False,name=\"Layer3\")(x)\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 16, kernel_size = (3, 3), strides=(2,2), kernel_initializer = initializer, \n",
        "      use_bias = False,name=\"Layer4\")(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 16, kernel_size = (3, 3), kernel_initializer = initializer,\n",
        "      use_bias = False,name=\"Layer5\")(x)\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 32, kernel_size = (3, 3), strides=(2,2), kernel_initializer = initializer, \n",
        "      use_bias = False,name=\"Layer6\")(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 32, kernel_size = (3, 3), kernel_initializer = initializer,\n",
        "      use_bias = False,name=\"Layer7\")(x)\n",
        "  x = tf.keras.layers.Conv2D(\n",
        "      filters = 64, kernel_size = (3, 3), strides=(2,2), kernel_initializer = initializer, \n",
        "      use_bias = False,name=\"Layer8\")(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=input,outputs=x)\n",
        "\n",
        "\n",
        "model = testmodel(tf.keras.initializers.HeUniform())\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=False, show_dtype=True)\n",
        "\n",
        "alpha = 0\n",
        "initializer = {\n",
        "    \"He-Uniform Initializer\":tf.keras.initializers.HeUniform(),\n",
        "    \"Glorot-Uniform Initializer\":tf.keras.initializers.GlorotUniform(),\n",
        "    \"Random-Normal Initializer\":tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02),\n",
        "    \"He-Normal Initializer\":tf.keras.initializers.HeNormal(),\n",
        "    \"Glorot-Normal Initializer\":tf.keras.initializers.GlorotNormal(),\n",
        "    \"He-Uniform Leaky\": HeNormalLeakyRelu(alpha)\n",
        "\n",
        "}\n",
        "models = [\n",
        "          testmodel(initializer[\"He-Uniform Initializer\"]),\n",
        "          testmodel(initializer[\"Glorot-Uniform Initializer\"]),\n",
        "          testmodel(initializer[\"Random-Normal Initializer\"]),\n",
        "          testmodel(initializer[\"He-Normal Initializer\"]),\n",
        "          testmodel(initializer[\"Glorot-Normal Initializer\"]),\n",
        "          testmodel(initializer[\"He-Uniform Leaky\"])\n",
        "\n",
        "]\n",
        "\n",
        "title = [\n",
        "         \"He-Uniform\",\n",
        "         \"Glorot-Uniform\" ,\n",
        "         \"Random-Normal Initializer\",\n",
        "         \"He-Normal\",\n",
        "         \"Glorot-Normal\",\n",
        "         \"He-Uniform Leaky: alpha={}\".format(alpha)]\n",
        "\n",
        "fig = plt.figure(figsize=[30,10])\n",
        "fig.suptitle(\"Initialized Weights before Training\", fontsize=20)\n",
        "\n",
        "for m, model in enumerate(models):\n",
        "  ax = fig.add_subplot(2, 3, m+1)\n",
        "  ax.set_title(title[m], fontsize=20)\n",
        "  for i, layer in enumerate(reversed(model.layers)): #reversed for pretier colors!\n",
        "    if \"Layer\" in layer.name:\n",
        "      ax.hist(np.ravel(layer.get_weights()),50,label=\"{}\".format(layer.name), alpha=0.5)\n",
        "  ax.legend(loc=\"upper left\",ncol=1)\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=False, show_dtype=False, rankdir='LR')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b_dh0-xwgf7"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "epochs = 51\n",
        "lr_setting = 1e-4\n",
        "warmup_epochs = 10\n",
        "epoch = np.arange(0,epochs,1)   # start,stop,step\n",
        "k = 2*np.pi/(epochs-warmup_epochs)\n",
        "warmup = np.where(\n",
        "    epoch < warmup_epochs, \n",
        "    (epoch+1)*(lr_setting/(warmup_epochs+1)), \n",
        "    lr_setting)\n",
        "warmup_exp_decay = np.where(\n",
        "    epoch < warmup_epochs, \n",
        "    (epoch+1)*(lr_setting/(warmup_epochs+1)), \n",
        "    lr_setting * np.exp(-k*(epoch+1-(warmup_epochs+1))))\n",
        "warmup_cosine_decay = np.where(\n",
        "    epoch < warmup_epochs, \n",
        "    (epoch+1)*(lr_setting/(warmup_epochs+1)), \n",
        "    (lr_setting/2) * np.cos((np.pi/(epochs-warmup_epochs))*(epoch-warmup_epochs))+(lr_setting/2))\n",
        "warmup_linear_decay = np.where(\n",
        "    epoch < warmup_epochs, \n",
        "    (epoch+1)*(lr_setting/(warmup_epochs+1)),\n",
        "     -(epoch-warmup_epochs)*(lr_setting/(epochs-(warmup_epochs)))+lr_setting)\n",
        "\n",
        "fig = plt.figure()\n",
        "fig = plt.figure(figsize=[20,10])\n",
        "fig.suptitle(\"Learning Rate over Epoch\", fontsize=30)\n",
        "\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "ax1.set_title('Epochs: {}, Warmup Epochs: {}, learning rate: {}'.format(epochs,warmup_epochs,lr_setting))\n",
        "ax1.plot(epoch, warmup, label=\"warmup\",linestyle=\"-\")\n",
        "ax1.plot(epoch, warmup_exp_decay, label=\"warmup_exp_decay\",linestyle=\"-\")\n",
        "ax1.plot(epoch, warmup_cosine_decay, label=\"warmup_cosine_decay\",linestyle=\"-\")\n",
        "ax1.plot(epoch, warmup_linear_decay, label=\"warmup_linear_decay\",linestyle=\"-\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Learning Rate\")\n",
        "ax1.legend(loc=\"center right\")\n",
        "ax1.grid(linestyle=':', linewidth=1,)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ravvTHOgPJu8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}